{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm7MWI5Ku-X1"
      },
      "source": [
        "<center>\n",
        "    <span style=\"color:blue; font-family:Georgia;  font-size:2em;\">\n",
        "        <h1>Recommender systems based on item-to-item  collaborative filtering</h1></span>\n",
        " </center>\n",
        "        <p> </p>\n",
        "        <p> </p>\n",
        "        <center><span style=\"color:blue; font-family:Georgia;  font-size:1em;\">\n",
        "        Ramon Béjar Torres</span></center>\n",
        "        <canvas id=\"myCanvas\" width=\"200\" height=\"100\" style=\"border:0px solid\"></canvas>\n",
        "        <center>Data mining - Master on Computer Science</center>\n",
        "        <center><img src=\"https://github.com/deinok/EXPLOTACIO-DE-DADES/blob/main/ASSIGNAMENT_2/M-UdL2.png?raw=1\"  width=\"200\" alt=\"UdL Logo\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK2ZS5PNu-X2"
      },
      "source": [
        "In this notebook we consider how to build recommender systems based on item-to-item (where items are the products of the on-line store) collaborative filtering.\n",
        "\n",
        "The approach to recommend new products to an user is based on finding SIMILAR items to:\n",
        "1. Products already bought by the user.\n",
        "2. Products we know the user likes or is interested in.\n",
        "\n",
        "So, the key issue in this kind of recommender systems is a way to compare two products, but based on the information from the set of users that bought those products.\n",
        "\n",
        "If similarity is based on user behaviour $\\rightarrow$ we expect similar products will be interesting **for users of that shop**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2HhlQhQu-X2"
      },
      "source": [
        "This approach is a good one when the user-item matrix represents which products were bought by each user, but the typical user row vector is sparse (there are few purchases with respect to the total number of items of the on-line store).\n",
        "\n",
        "The most extreme case is when a 'new user' enters to our shop, so we do not have any purchases yet for that user.\n",
        "\n",
        "This is related to the so called 'cold start problem' in recommender systems!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6QFI4o2u-X3"
      },
      "source": [
        "This product-centered approach allows us to compute recommendations based on any single product bought by any user.\n",
        "\n",
        "Even in the extreme case where the user has only bought one producty, the system will be able to compute good recommendations if the **total number of purchases of the on-line store is high enough**.\n",
        "\n",
        "In case the user did not buy any products, we can consider products he has browsed in the online store frequently, and then find similar products to these ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGpEUfHFu-X3"
      },
      "source": [
        "In any case, the main issue here will be **how to compare products of the on-line store** and how to predict wich ones will be more interesting for the user.\n",
        "\n",
        "So, with respect to the approach based on latent factors:\n",
        "- Here we also perform global filtering because we consider the whole set of users and products to build the system\n",
        "- But we compute explicitely similarity measures between any pair of products, instead of decomposing users and products as vectors of latent factors to predict the matching between users and products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSFw3hCXu-X3"
      },
      "source": [
        "Preliminary start-up code for the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JQesyTg7QSU-",
        "outputId": "10fb6da4-a436-4717-b08b-76a2f0efd2cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core ca-certificates-java default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra gsettings-desktop-schemas java-common libatk-bridge2.0-0\n",
            "  libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0 libatk1.0-data\n",
            "  libatspi2.0-0 libpcsclite1 libxcomposite1 libxtst6 libxxf86dga1\n",
            "  openjdk-11-jre openjdk-11-jre-headless session-migration x11-utils\n",
            "Suggested packages:\n",
            "  pcscd libnss-mdns fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core ca-certificates-java default-jre default-jre-headless\n",
            "  fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas java-common\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libpcsclite1 libxcomposite1 libxtst6\n",
            "  libxxf86dga1 openjdk-11-jre openjdk-11-jre-headless session-migration\n",
            "  x11-utils\n",
            "0 upgraded, 22 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 46.5 MB of archives.\n",
            "After this operation, 190 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 java-common all 0.72build2 [6,782 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcsclite1 amd64 1.9.5-3ubuntu1 [19.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ca-certificates-java all 20190909ubuntu1.2 [12.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Fetched 46.5 MB in 3s (16.6 MB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121235 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package java-common.\n",
            "Preparing to unpack .../05-java-common_0.72build2_all.deb ...\n",
            "Unpacking java-common (0.72build2) ...\n",
            "Selecting previously unselected package libpcsclite1:amd64.\n",
            "Preparing to unpack .../06-libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
            "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../07-openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../08-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package ca-certificates-java.\n",
            "Preparing to unpack .../09-ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
            "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../10-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../11-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../12-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../13-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../14-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../15-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../16-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../17-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../18-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../19-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../20-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../21-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up java-common (0.72build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
            "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
            "Adding debian:AffirmTrust_Networking.pem\n",
            "Adding debian:UCA_Extended_Validation_Root.pem\n",
            "Adding debian:e-Szigno_Root_CA_2017.pem\n",
            "Adding debian:Amazon_Root_CA_2.pem\n",
            "Adding debian:QuoVadis_Root_CA_2.pem\n",
            "Adding debian:XRamp_Global_CA_Root.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
            "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
            "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
            "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
            "Adding debian:Starfield_Class_2_CA.pem\n",
            "Adding debian:AffirmTrust_Commercial.pem\n",
            "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
            "Adding debian:GlobalSign_Root_R46.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority.pem\n",
            "Adding debian:Amazon_Root_CA_3.pem\n",
            "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
            "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
            "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
            "Adding debian:GlobalSign_Root_CA.pem\n",
            "Adding debian:GTS_Root_R2.pem\n",
            "Adding debian:ACCVRAIZ1.pem\n",
            "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
            "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
            "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
            "Adding debian:certSIGN_Root_CA_G2.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
            "Adding debian:certSIGN_ROOT_CA.pem\n",
            "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
            "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
            "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
            "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
            "Adding debian:emSign_Root_CA_-_G1.pem\n",
            "Adding debian:Security_Communication_RootCA3.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
            "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
            "Adding debian:UCA_Global_G2_Root.pem\n",
            "Adding debian:DigiCert_Global_Root_G2.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
            "Adding debian:ISRG_Root_X1.pem\n",
            "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:TWCA_Root_Certification_Authority.pem\n",
            "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
            "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:SecureSign_RootCA11.pem\n",
            "Adding debian:ISRG_Root_X2.pem\n",
            "Adding debian:Go_Daddy_Class_2_CA.pem\n",
            "Adding debian:Certum_Trusted_Root_CA.pem\n",
            "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
            "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
            "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
            "Adding debian:DigiCert_Global_Root_CA.pem\n",
            "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
            "Adding debian:Certigna.pem\n",
            "Adding debian:QuoVadis_Root_CA_3.pem\n",
            "Adding debian:Secure_Global_CA.pem\n",
            "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
            "Adding debian:GTS_Root_R4.pem\n",
            "Adding debian:SecureTrust_CA.pem\n",
            "Adding debian:GlobalSign_Root_E46.pem\n",
            "Adding debian:Certum_Trusted_Network_CA.pem\n",
            "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
            "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
            "Adding debian:GLOBALTRUST_2020.pem\n",
            "Adding debian:COMODO_Certification_Authority.pem\n",
            "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
            "Adding debian:GTS_Root_R3.pem\n",
            "Adding debian:Buypass_Class_2_Root_CA.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
            "Adding debian:Izenpe.com.pem\n",
            "Adding debian:Certainly_Root_R1.pem\n",
            "Adding debian:Certigna_Root_CA.pem\n",
            "Adding debian:AffirmTrust_Premium_ECC.pem\n",
            "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
            "Adding debian:TunTrust_Root_CA.pem\n",
            "Adding debian:Amazon_Root_CA_4.pem\n",
            "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
            "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
            "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
            "Adding debian:Certum_EC-384_CA.pem\n",
            "Adding debian:Security_Communication_RootCA2.pem\n",
            "Adding debian:Certainly_Root_E1.pem\n",
            "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
            "Adding debian:Actalis_Authentication_Root_CA.pem\n",
            "Adding debian:AffirmTrust_Premium.pem\n",
            "Adding debian:emSign_Root_CA_-_C1.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
            "Adding debian:SZAFIR_ROOT_CA2.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
            "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
            "Adding debian:vTrus_Root_CA.pem\n",
            "Adding debian:Telia_Root_CA_v2.pem\n",
            "Adding debian:ePKI_Root_Certification_Authority.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
            "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
            "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
            "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
            "Adding debian:TWCA_Global_Root_CA.pem\n",
            "Adding debian:GTS_Root_R1.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
            "Adding debian:Comodo_AAA_Services_root.pem\n",
            "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
            "Adding debian:vTrus_ECC_Root_CA.pem\n",
            "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
            "Adding debian:DigiCert_Global_Root_G3.pem\n",
            "Adding debian:CA_Disig_Root_R2.pem\n",
            "Adding debian:Baltimore_CyberTrust_Root.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
            "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
            "Adding debian:Amazon_Root_CA_1.pem\n",
            "Adding debian:Buypass_Class_3_Root_CA.pem\n",
            "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
            "Adding debian:CFCA_EV_ROOT.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:Atos_TrustedRoot_2011.pem\n",
            "Adding debian:Security_Communication_Root_CA.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
            "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
            "Adding debian:BJCA_Global_Root_CA1.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
            "Adding debian:BJCA_Global_Root_CA2.pem\n",
            "done.\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
            "Updating certificates in /etc/ssl/certs...\n",
            "0 added, 0 removed; done.\n",
            "Running hooks in /etc/ca-certificates/update.d...\n",
            "\n",
            "done.\n",
            "done.\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install default-jre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgYcNbH-u-X3",
        "outputId": "132e3eff-f908-4447-f594-56ccc09bb8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/share/spark-3.0.1-bin-hadoop2.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10.21.167.77:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import os\n",
        "import math\n",
        "import sys\n",
        "from pyspark.mllib.linalg import SparseVector\n",
        "# from pyspark.mllib.linalg import DenseVector\n",
        "\n",
        "spark_home = os.environ.get('SPARK_HOME', None)\n",
        "print (spark_home)\n",
        "\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AsTDtkmu-X4"
      },
      "source": [
        "## Comparing products based on set of common costumers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "UzxtdwMSu-X4"
      },
      "source": [
        "The first step towards discovering (mining) similar products is how to measure the similarity between two products in the store catalog.\n",
        "\n",
        "If we assume that the information we have for every product is the set of users that bought that product, the global filtering approach is based on comparing ALL the costumers of the store, checking how many costumers have bought both the two items we want to compare.\n",
        "\n",
        "> The idea is that the similarity measure should be higher when the two products have a higher number of common costumers.\n",
        "\n",
        "So, a very direct way of measuring the distance would be to count the number of common costumers, and normalize it by the total number of costumers of the store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E86Ec4u-X4"
      },
      "source": [
        "### The cosine similarity\n",
        "\n",
        "However, when we consider user-product matrices that incorporate more complex information about user-product pairs, for example the satisfaction degree of the costumer with the product, it is better to consider other kinds of similarity measures. One widely used is the **cosine similarity**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1SajRqDu-X5"
      },
      "source": [
        "Given the angle $\\theta$ between two product vectors v1 and v2, the cosine similarity is defined as the cosine of the angle $\\theta$, that we can compute from the components of both vectors as follows:\n",
        "\n",
        "$$ cossim(v_1,v_2) = cos(\\theta) = \\frac{v_1 \\cdot v_2}{\\sqrt{\\sum_i v_1[i]^2}\\sqrt{\\sum_i v_2[i]^2}} = \\frac{\\sum_i (v_1[i]*v_2[i])}{\\sqrt{\\sum_i v_1[i]^2}\\sqrt{\\sum_i v_2[i]^2}} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJtoCwQXu-X5"
      },
      "source": [
        "This comes from the formula for the dot product for two vectors:\n",
        "$$ {\\displaystyle \\mathbf {v_1} \\cdot \\mathbf {v_2} =\\left\\|\\mathbf {v_1} \\right\\|\\left\\|\\mathbf {v_2} \\right\\|\\cos \\theta } $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3fPIn_u-X5"
      },
      "source": [
        "Some key properties:\n",
        "- The value ranges in [-1,1]\n",
        "- -1 means they are opposite vectors ($\\theta=180$)\n",
        "- 0 means they are orhogonal vectors ($\\theta=90$)\n",
        "- 1 means they are equal or proportional (same direction) ($\\theta=0$)\n",
        "\n",
        "It only makes sense to compute this measure when both vectors are non-zero (the module of any of them is not zero). Otherwise, we cannot define a clear angle between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB6rEmCKu-X5"
      },
      "source": [
        "So, when used as a similarity measure, observe the cosine similarity can signal from totally different products (-1) to totally similar ones (1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44YX2Fjdu-X5"
      },
      "source": [
        "For working on product-to-product based collaborative filtering, we must work with the columns of the user-product matrix, the same matrix that we used with global filtering based on finding latent factors.\n",
        "\n",
        "From now on, we will assume that products are already given as vectors, representing their corresponding column vectors in the user-product matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHfYDiuXu-X5"
      },
      "outputs": [],
      "source": [
        "#  Compute the cosine similarity between vectors vec1 and vec2, represented\n",
        "#  as dense lists: with all the elements (non-zero and zero) values present\n",
        "def CosineSimilarity( vec1, vec2 ):\n",
        "    dot = 0.0\n",
        "    v1rs = 0.0\n",
        "    v2rs = 0.0\n",
        "    for i in range(len(vec1)):\n",
        "        dot += (vec1[i]*vec2[i])\n",
        "        v1rs += (vec1[i]*vec1[i])\n",
        "        v2rs += (vec2[i]*vec2[i])\n",
        "    v1rs = math.sqrt(v1rs)\n",
        "    v2rs = math.sqrt(v2rs)\n",
        "    return dot/(v1rs*v2rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_LZwdzpu-X5"
      },
      "source": [
        "However, working with sparse vectors is a better approach if we consider that the fraction of users that have bought a product will be low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN9v2p2gu-X6"
      },
      "source": [
        "###  Binary feature vectors\n",
        "\n",
        "Consider for example the following set of 4 product vectors, each one indicating which costumers, from a total set of 4 costumers, have bought (1) or have not bought (0) the product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u91gA3lJu-X6"
      },
      "outputs": [],
      "source": [
        "itemvectors = [ [1,1,0,1,1], [1,1,0,1,1], [0,1,0,0,1], [1,0,1,1,0] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7CMAhjbu-X6",
        "outputId": "5be31668-1d9f-4fd8-8b90-ea937ab62e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity between  0 and 1 : 1.0\n",
            "Cosine similarity between  0 and 2 : 0.7071067811865475\n",
            "Cosine similarity between  0 and 3 : 0.5773502691896258\n",
            "Cosine similarity between  1 and 2 : 0.7071067811865475\n",
            "Cosine similarity between  1 and 3 : 0.5773502691896258\n",
            "Cosine similarity between  2 and 3 : 0.0\n"
          ]
        }
      ],
      "source": [
        "for i1, vec1 in enumerate(itemvectors):\n",
        "    for i2, vec2 in enumerate(itemvectors):\n",
        "        if (i1 < i2):\n",
        "          print (\"Cosine similarity between \", i1, \"and\", i2, \":\", CosineSimilarity( vec1, vec2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7pkSVwAu-X6"
      },
      "source": [
        "### Negative and positive features vectors\n",
        "\n",
        "As we have said, the cosine distance is also defined when our product vectors contain values in a range that includes negative values. This is the case where:\n",
        "* negative values mean *negative* ratings\n",
        "* positive values mean positive ratings\n",
        "* 0 mean a neutral rating (or no rating at all). For example, consider the following set of products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgyHrjhdu-X6"
      },
      "outputs": [],
      "source": [
        "itemvectors2 = [ [-3,-3,-3,-2,2], [-2,-2,-2,-1,2], [1,1,1,1,-2], [3,3,3,3,-6] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_UxVWWOu-X6",
        "outputId": "d5888539-ce28-477c-e043-6d9a862a4f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance between  0 and 1 : 0.9839040740289453\n",
            "Cosine distance between  0 and 2 : -0.8964214570007952\n",
            "Cosine distance between  0 and 3 : -0.8964214570007952\n",
            "Cosine distance between  1 and 2 : -0.9432422182837986\n",
            "Cosine distance between  1 and 3 : -0.9432422182837986\n",
            "Cosine distance between  2 and 3 : 1.0\n"
          ]
        }
      ],
      "source": [
        "for i1, vec1 in enumerate(itemvectors2):\n",
        "    for i2, vec2 in enumerate(itemvectors2):\n",
        "        if (i1 < i2):\n",
        "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineSimilarity( vec1, vec2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuwdiI8iu-X7"
      },
      "source": [
        "Observe that in this case, the cosine distance ranges from -1 to 1, where -1 means *totally opposite vectors*, and 1 means totally aligned vectors, without giving relevance to the magnitude of the vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEBENlAqu-X7"
      },
      "source": [
        "## Similarity in the users-movies example\n",
        "\n",
        "Let's consider what information the cosine distance provides when comparing movies in our users-movies example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH63NBsdu-X7"
      },
      "outputs": [],
      "source": [
        "# We have 10 users, and 10 movies: STW1, STW2, STW3, STW4, STW5, STW6\n",
        "#                                  T1, T2, T3 and BaT\n",
        "usersandmovies = [ [3,3,3,5,5,4, 3,3,-1, -1], \\\n",
        "                   [3,3,3,5,5,4, 4,2,0, -1], \\\n",
        "                   [3,3,4,5,5,4, 4,4,1, 0], \\\n",
        "                   [4,3,3,4,5,4, 3,3,1, -1], \\\n",
        "                   [1,1,1,0,1,1, 5,4,2, -1], \\\n",
        "                   [1,2,1,0,1,1, 4,4,2, -1], \\\n",
        "                   [1,2,2,1,1,1, 4,4,2, -1], \\\n",
        "                   [1,2,2,1,1,0, 5,4,3, -1], \\\n",
        "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4], \\\n",
        "                   [-2,-3,-2,0,-2,-1, 0,0,-1,4]   ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSKgRi_Yu-X7"
      },
      "outputs": [],
      "source": [
        "def getMovieVector( usersandmovies, j ):\n",
        "    return [ usersandmovies[u][j] for u in range(len(usersandmovies))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCz31E8hu-X7"
      },
      "source": [
        "Similarity between the first four star wars movies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSQXqVD5u-X7",
        "outputId": "23eee9fe-78f5-4002-dcd1-81007f40ca43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance between  0 and 1 : 0.9554528219538876\n",
            "Cosine distance between  0 and 2 : 0.9668114046189884\n",
            "Cosine distance between  0 and 3 : 0.8808819893600309\n",
            "Cosine distance between  1 and 2 : 0.9698160576680458\n",
            "Cosine distance between  1 and 3 : 0.772771255454225\n",
            "Cosine distance between  2 and 3 : 0.8762691935871222\n"
          ]
        }
      ],
      "source": [
        "swmovies = [ getMovieVector( usersandmovies, j ) for j in range(4)]\n",
        "for i1, vec1 in enumerate(swmovies):\n",
        "    for i2, vec2 in enumerate(swmovies):\n",
        "        if (i1 < i2):\n",
        "          print (\"Cosine distance between \", i1, \"and\", i2, \":\", CosineSimilarity( vec1, vec2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7titYnru-X8"
      },
      "source": [
        "Similarity between terminator movies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PQLYDHgu-X8",
        "outputId": "00331928-8a37-4597-bc4c-f13a3e94d721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance between  6 and 7 : 0.9824666109905364\n",
            "Cosine distance between  6 and 8 : 0.7681373347487839\n",
            "Cosine distance between  7 and 8 : 0.7767356373806175\n"
          ]
        }
      ],
      "source": [
        "tmovies = [ getMovieVector( usersandmovies, j ) for j in [6,7,8]]\n",
        "for i1, vec1 in enumerate(tmovies):\n",
        "    for i2, vec2 in enumerate(tmovies):\n",
        "        if (i1 < i2):\n",
        "          print (\"Cosine distance between \", 6+i1, \"and\", 6+i2, \":\", CosineSimilarity( vec1, vec2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAd2qIk8u-X8"
      },
      "source": [
        "Similarity between the first four star wars movies and terminator movies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rda29y4Gu-X8",
        "outputId": "47271a1c-b1f1-4ba7-8e7f-662679cdfe4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance between  0 and 6 : 0.7393877297305068\n",
            "Cosine distance between  0 and 7 : 0.73431307102251\n",
            "Cosine distance between  0 and 8 : 0.4495530025457533\n",
            "Cosine distance between  1 and 6 : 0.7762444233578819\n",
            "Cosine distance between  1 and 7 : 0.7741809609998153\n",
            "Cosine distance between  1 and 8 : 0.5989849814784503\n",
            "Cosine distance between  2 and 6 : 0.8135251376128875\n",
            "Cosine distance between  2 and 7 : 0.8113625732861212\n",
            "Cosine distance between  2 and 8 : 0.552422157047008\n",
            "Cosine distance between  3 and 6 : 0.6859384573602509\n",
            "Cosine distance between  3 and 7 : 0.6673778622698389\n",
            "Cosine distance between  3 and 8 : 0.18302666282596894\n"
          ]
        }
      ],
      "source": [
        "for i1,vec1 in enumerate(swmovies):\n",
        "    for i2,vec2 in enumerate(tmovies):\n",
        "        print (\"Cosine distance between \", i1, \"and\", 6+i2, \":\", CosineSimilarity( vec1, vec2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbVTT2XJu-X9"
      },
      "source": [
        "So observe that many star wars movies can be considered to be *similar* to many terminator movies (from the point of view of our set of users). But not all such pairs are clearly similar. For example:\n",
        "- Similarity for (SW1,T2) : 0.73\n",
        "- Similarity for (SW4,T3) : 0.18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBS3CYJru-X9"
      },
      "source": [
        "Similarity between Star Wars I and Breakfast at Tiffanys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynRw4Eecu-X9",
        "outputId": "1f4d0b9a-bf38-43ac-cfa4-f4c78646800e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance between 0 and 8 : -0.6477502756312957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/11/05 17:35:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "vec1, vec2 = getMovieVector( usersandmovies, 0 ), getMovieVector( usersandmovies, 9 )\n",
        "print (\"Cosine distance between 0 and 8 :\", CosineSimilarity( vec1, vec2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiOWYNYPu-X9"
      },
      "source": [
        "So, in this case we get a negative value, because it seems that Star Wars and Breakfast at Tiffanys are very opposite movies...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As9TIbg6u-X9"
      },
      "source": [
        "## Mining similar items based on item-to-item global filtering\n",
        "\n",
        "Let's now consider the approach for recommending products to the Amazon costumers presented in the paper:\n",
        "\n",
        "> Greg Linden, Brent Smith, and Jeremy York. *Amazon.com recommendations: Item-to-Item Collaborative Filtering*.\n",
        "> In IEEE INTERNET COMPUTING. 2003\n",
        "\n",
        "In that paper you will find only the overall idea. The approach is the one we have explained before, use some similarity measure between products to recommend relevant products. We do not know what are the current similarity measures used by Amazon, but we will use the cosine distance in this notebook to develop our recommender system.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ot47W1u-X9"
      },
      "source": [
        "The main building block of the Amazon recomender system is their algorithm to compute similarity between any pair of items (products) in their on-line catalog. This is the pseudo-code of the mentioned Amazon item-to-item similarity mining algorithm:\n",
        "\n",
        "```python\n",
        "def computeSimilarityBetweenProducts( I ):\n",
        "   for each item i1 in product catalog I:\n",
        "      for each customer C who purchased i1:\n",
        "         for each item i2 (not = i1) purchased by customer C:\n",
        "             # record that *a customer* (C) purchased i1 and i2:\n",
        "             store that (i1,i2) were purchased by a same user  \n",
        "      ## The next loop can really be reduced to items i2 such that  \n",
        "      ## (i1,i2) was recorded at least once      \n",
        "      for each item i2 in product catalog I:\n",
        "        compute the similarity between i1 and i2\n",
        "```\n",
        "\n",
        "This algorithm can be though as an **off-line** algorithm, the similarity between products should be computed as a background process, and only be recomputed when there are a significant number of changes in the purchases database. What is the worst-case and real complexity of this algorithm ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjsX4eR0u-X-"
      },
      "source": [
        "Consider the following observations (where $M$ is the number of costumers and $N$ the number of items of the on-line store:\n",
        "\n",
        "1. The worst-case complexity is $ O(N^{2}  M)$. This is the case when almost any user has bought any item of the store.  \n",
        "2. However, if we assume that many costumers have very few purchases (let's say a constant number), the real complexity is more closer to $O(N \\ M)$.\n",
        "3. The complexity can be further reduced if we only consider a sample (subset) of costumers to compute the similarity between products. Of course, this will produce an approximation of the real similarity values between products.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L2vy7Hcu-X-"
      },
      "source": [
        "For computing the similarity, we can use the cosine similarity, or any other similarity measure we think is good for our application domain. Observe that in some sense, it is reasonable to think that:\n",
        "\n",
        "> the similarity between a pair of products (i1,i2) will be *a number* proportional to the total number of customers that purchased both i1 and i2,\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9x8m77Yu-X-"
      },
      "source": [
        "### Distributed computation\n",
        "\n",
        "If we only record whether an user buys a product (1/0 binary feature vectors), then observe that the similarity measure depends only on the total number of such customers, and not the  particular ones.\n",
        "\n",
        "So, the computation needed to compute the similarity between i1 and i2 can be thought as some kind of *reduce* (by Key) operation between all pairs (i1,i2) produced by different users C. That is, if we consider (i1,i2) as the key, and for example \"1\" as the value for each different user C that bought i1 and i2, the (key,value) to produce would be:\n",
        "\n",
        "          ((i1,i2), 1)  for each user C that bought i1 and i2\n",
        "\n",
        "and then reduce by key (for example summing up the values) all such (key,value) pairs. Of course, to get a normalized similarity measure the sum should be divided by the maximun number of users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_8Akuneu-X-"
      },
      "source": [
        "The output format of the data set computed by such global filtering algorithm could be something like the following. For each item i we have a list of (item,similarity) pairs:\n",
        "  \n",
        " i :  [ ($j_1$,sim(i,$j_1$)), ($j_2$,sim(i,$j_2$)), ..., ($j_l$,sim(i,$j_l$)) ]\n",
        "\n",
        " where the set of items $j_1$, $j_2$, ... , $j_l$ is the set of items that have at least one common costumer (user) with i and so their similarity is > 0. We will refer to the (distributed) data set that contains such information for all the items as the **rddSimilarityPairs** in the rest of this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F3hH8wSu-X-"
      },
      "source": [
        "If we instead consider the distributed computation of the cosine similarity, in the more general case with negative and positive feature values, the computation could be as follows:\n",
        "1. Map every pair of user-product ratings **with the same user u** to the values they contribute in the final cosine similarity between p1 and p2:\n",
        "$$  (u,p_1,r_1),(u,p_2,r_2) \\rightarrow ((p_1,p_2),(r_1 r_2,r_1^2,r_2^2) ) $$\n",
        "2. Reduce all the previous key-value pairs, with the same key as:\n",
        "$$ ((p_1,p_2),(pra_{1,2},ra_1^2,ra_2^2) ) + ((p_1,p_2),(prb_{1,2},rb_1^2,rb_2^2) )\n",
        "   \\rightarrow \\\\  ((p_1,p_2),( pra_{1,2}+prb_{1,2}, ra_1^2+rb_1^2,\n",
        "   ra_2^2+rb_2^2) ) $$\n",
        "3. Compute the cosine similarity combining the reduced values in a final map:\n",
        "$$ ((p_1,p_2),(\\sum_u r_1 r_2,\\sum_u r_1^2,\\sum_u r_2^2) ) \\rightarrow\n",
        "\\frac{\\sum_u r_1 r_2}{\\sqrt{\\sum_u r_1^2} \\sqrt{\\sum_u r_2^2}}  $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVDzNHx2u-X-"
      },
      "source": [
        "Observe that the previous solution is not necessarily the most efficient way to compute the cosine similarity:\n",
        "\n",
        "* It produces redundant information: info for $(p_1,p_2)$ will be the same one as the info for $(p_2,p_1)$. This could be fixed filtering the pairs we combine in step 1 to those with $p_1 < p_2$.\n",
        "\n",
        "* The sum of squares $ \\sum_u r_i^2$ for item $i$ is computed as many times as different pairs of items we build with $i$. So, to save some computation, the term $ \\sum_u r_i^2$ could be instead computed only once, and then reused when computing the cosine similarity between item $i$ and any other item $j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxl4itsPu-X-"
      },
      "source": [
        "## Recommending similar items to a previously bought one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qSOUD7ju-X_"
      },
      "source": [
        "As a first recommender system, consider the case where we want to recommend similar items to one just bought by an user, or to one recently browsed by the user. So, we want to focus on similar items to a particular one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmGtl64Ru-X_"
      },
      "source": [
        "Here you have a possible pseudo-code for a recommender system for that particular case. The input is the user we are considering and the item we want to use as the base item to get the recommendations. Observe that the primary use of this algorithm would be \"on-line\": every time an user makes a new purchase or browses a new item it would be desirable to get recommendations focused on that new item.\n",
        "\n",
        "```python\n",
        "def RecommendKmostSimilar( RDD rddSimilarityPairs, user U, item I, int K ):\n",
        "     rdd1 = getSimilarItems( rddSimilarityPairs, I )\n",
        "     rdd2 = EraseItemsAlreadyBought( rdd1, U )\n",
        "     rdd3 = rdd2.sortBySimilarity()\n",
        "     bestK = rdd3.takeFirstKItems( K )\n",
        "     \n",
        "     return bestK\n",
        "```\n",
        "\n",
        "This function assumes that we have a previously computed data set, the rddSimilarityPairs, that should be the one computed by the similarity mining algorithm of  the previous section (or by any other algorithm that provides such data set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIKyRbjPu-X_"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "Let's see a possible implementation in spark of the four steps of the previous algorithm. Let's first consider the following similarity information data set, that for simplicity we will consider that is stored in a plain ASCII file. In a final application this similarity information would be stored in a database.\n",
        "\n",
        "Observe that if we assume that the similarity between products does not change significantly very frequently, we can compute the similarity with a **off-line algorithm**, and run on-line with an RDD that contains only the similarity between the current target product (item I in the previous algorithm) and the other products with similarity to I greater than zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LTbmoGxu-X_"
      },
      "source": [
        "Information found on file similarityPairsInfo_1.txt:\n",
        "\n",
        "1    (2,0.6)  (4,0.3)\n",
        "\n",
        "2    (1,0.6)\n",
        "\n",
        "3    (4,0.7)  (5,0.8)\n",
        "\n",
        "4    (3,0.7)  (5,0.4) (1,0.3)\n",
        "\n",
        "5    (3,0.8)  (4,0.4)\n",
        "\n",
        "6    (7,0.3)\n",
        "\n",
        "7    (6,0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRu1BWRKu-X_"
      },
      "outputs": [],
      "source": [
        "# Format of line:  ItemI    ItemI1,Sim_(I,I1) ...  ItemIN,Sim_(I,IN)\n",
        "#\n",
        "# We assume that the line contains only the information for items such that their similarity with I is > 0\n",
        "#\n",
        "def parseSimilarityInfo( line ):\n",
        "   toks = line.split()\n",
        "   sourceitem = int(toks[0])\n",
        "   targetitems = [ tuple(it.split(',')) for it in toks[1:] ]\n",
        "   targetitems = [ (int(it[0]),float(it[1])) for it in targetitems ]\n",
        "   return (sourceitem,targetitems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FDWJL1Su-X_"
      },
      "source": [
        "### 0. Load similarity info from file to RDD  \n",
        "\n",
        "Before computing recommendations, we load the file similarityPairsInfo_1.txt to get the desired RDD\n",
        "data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHtzQCZCu-YA"
      },
      "outputs": [],
      "source": [
        "rddSimilarityPairs = sc.textFile('similarityPairsInfo_1.txt').map( parseSimilarityInfo )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh-BzCx1u-YA"
      },
      "source": [
        "Let's take a look to check if the file was well parsed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddZdWi2Fu-YA",
        "outputId": "260b22fc-1da1-4d78-af50-c588b13a8726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "[Stage 0:>                                                          (0 + 2) / 2]\r",
            "\r",
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(1, [(2, 0.6), (4, 0.3)]),\n",
              " (2, [(1, 0.6)]),\n",
              " (3, [(4, 0.7), (5, 0.8)]),\n",
              " (4, [(3, 0.7), (5, 0.4), (1, 0.3)]),\n",
              " (5, [(3, 0.8), (4, 0.4)]),\n",
              " (6, [(7, 0.3)]),\n",
              " (7, [(6, 0.3)])]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rddSimilarityPairs.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kjzXNugu-YA"
      },
      "source": [
        "Observe that this format for storing similarity info is efficient towards finding the complete info for an item, but has some redundancy in the information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvIgw6-gu-YA"
      },
      "source": [
        "### 1. Filter similarity info related to item I\n",
        "\n",
        "Next, we can filter from such rdd data set only the similarity information related to our input item I:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wInDEptSu-YA"
      },
      "outputs": [],
      "source": [
        "def getSimilarItems( rddSimilarityPairs, I ):\n",
        "   return  rddSimilarityPairs.filter( lambda x : x[0] == I )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "_vrR7jv3u-YB"
      },
      "source": [
        "Let's test the function with item 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyEnAQEYu-YB",
        "outputId": "2aa5ef55-9a03-4ef3-9324-cfd9ce2c7043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Item 4 Sim Info:  [(4, [(3, 0.7), (5, 0.4), (1, 0.3)])]\n"
          ]
        }
      ],
      "source": [
        "rdd1 = getSimilarItems( rddSimilarityPairs, 4 )\n",
        "item4Info = rdd1.collect()\n",
        "print(\" Item 4 Sim Info: \", item4Info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEKE85Uqu-YB"
      },
      "source": [
        "### 2. Filter elements not already bought by user U\n",
        "\n",
        "The next step is to retain only items not bought by user U. Again, the implementation is highly dependent on whether we assume the set of purchases of user U fits into a single machine or it must be distributed. Let's assume here that his/her ser of purchases fits into one machine, so we can read it into the following function. We assume this format:\n",
        "\n",
        "   user1  item11   item12 ...  item1N\n",
        "   \n",
        "   user2  item11   item12 ...  item1N\n",
        "   \n",
        "    ...\n",
        "    \n",
        "   userN  item11   item12 ...  item1N\n",
        "   \n",
        "where the set of items in line i is the set of items bought by user i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clAnEWRku-YB"
      },
      "outputs": [],
      "source": [
        "# In this function we remove the set of already items by U, and also\n",
        "# remove the item key to get the final set\n",
        "# similar and filtered items as a single list\n",
        "def RemoveBoughtItems( itemsSimilarToI, U ):\n",
        "   purchases = getPurchases( U )\n",
        "   print ( \" Purchases by user \", U, \" : \", purchases )\n",
        "   return itemsSimilarToI.flatMap( lambda x : [it for it  in x[1] if it[0] not in purchases ] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poMA97GFu-YB"
      },
      "source": [
        "We will consider the next example file of user purchases ('purchases.txt'):\n",
        "\n",
        "    1    2  3\n",
        "    2    3  4\n",
        "    3    4  5  6\n",
        "    4    6  1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuxPWBSDu-YB"
      },
      "outputs": [],
      "source": [
        "def getPurchases( U ):\n",
        "    purchases = []\n",
        "    f = open( 'purchases.txt' )\n",
        "    for line in f:\n",
        "        toks = line.split()\n",
        "        if (int(toks[0]) == U):\n",
        "            purchases = toks[1:]\n",
        "            break\n",
        "    f.close()\n",
        "    return [ int(p) for p in purchases ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ImL3QqiSu-YC"
      },
      "source": [
        "Let's check it filtering out the purchases of user 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX43El1Du-YC",
        "outputId": "4232921f-d17b-4e23-9d7c-f8df82de7131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Purchases by user  3  :  [4, 5, 6]\n",
            " Item 4 Sim Info:  [(4, [(3, 0.7), (5, 0.4), (1, 0.3)])]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(3, 0.7), (1, 0.3)]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd2 = RemoveBoughtItems( rdd1, 3 )\n",
        "print(\" Item 4 Sim Info: \", item4Info)\n",
        "rdd2.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mrsUmi6u-YC"
      },
      "source": [
        "### 3. Sort them and take the most K similar items\n",
        "\n",
        "The last two steps are sort the resulting set of items by similarity and taking the first k items. We can do this with spark with a single action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_8uskkDu-YC",
        "outputId": "609f0eb6-6333-41b1-c9e3-dde61f32ea5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(3, 0.7)]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Take only the first most similar non-bought item\n",
        "rdd2.takeOrdered(1, key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBYMOdDHu-YC"
      },
      "source": [
        "# The meaning of empty entries\n",
        "\n",
        "Observe that in the cosine similarity formula:\n",
        "$$ cossim(v_1,v_2) = cos(\\theta) = \\frac{v_1 \\cdot v_2}{\\sqrt{\\sum_i v_1[i]^2}\\sqrt{\\sum_i v_2[i]^2}} $$\n",
        "\n",
        "the contribution of user i will be 0 if the entries $ v_1[i] $ and $ v_2[i]$ are equal to 0. That is, it would be like simplifying the vectors eliminating the entry $i$.\n",
        "\n",
        "But then, in case the values represent ratings like in the movies example, what should be the contribution of that entry if the value is not 0, but empty (user i did not give ratings for movies $v_1$ and $v_2$)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5JV2wtnu-YC"
      },
      "source": [
        "## Assuming 0 means empty rating\n",
        "\n",
        "For example, assume the two modified vectors for star wars 1 and star wars 4, where the 0 represents NO rating (instead of neutral ratings):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP--q8DBu-YD",
        "outputId": "25179fbf-093c-453b-ab3e-3c4586f52eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity between sw1  [3, 3, 3, 4, 0, 0, 1, 1, -2, -2] and sw4  [5, 5, 5, 4, 0, 0, 1, 1, 0, 0] : 0.8973484983270362\n"
          ]
        }
      ],
      "source": [
        "sw1 = getMovieVector( usersandmovies, 0 )\n",
        "sw4 = getMovieVector( usersandmovies, 3 )\n",
        "sw1[4] = sw1[5] = 0\n",
        "print (\"Cosine Similarity between sw1 \", sw1, \"and sw4 \", sw4, \":\", CosineSimilarity( sw1, sw4 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gz_Pgftu-YD"
      },
      "source": [
        "If we increase the empty entries that coincide for a same user:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMsupOYTu-YD",
        "outputId": "58f80844-7bb4-4706-e06b-6ad428cea02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity between sw1  [3, 3, 3, 4, 0, 0, 1, 1, 0, 0] and sw4  [5, 5, 5, 4, 0, 0, 1, 1, 0, 0] : 0.9738516810963532\n"
          ]
        }
      ],
      "source": [
        "sw1[8] = sw1[9] = 0\n",
        "print (\"Cosine Similarity between sw1 \", sw1, \"and sw4 \", sw4, \":\", CosineSimilarity( sw1, sw4 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql7zVoRHu-YD"
      },
      "source": [
        "So, having less ratings can induce a higher value for the cosine similarity, when the non-empty entries are very similar, even if they are very few with respect to the total number of users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PALp9lzGu-YD"
      },
      "source": [
        "## Recommending based on the whole set of previous purchases\n",
        "\n",
        "Sometimes Amazon also sends emails to costumers sending recommendations based on *all* (or a subset of) his/her previous purchases, instead of based on a single recent one.\n",
        "\n",
        "Observe that in this case, a same item could be recommended but with different similarity values, as the result of being similar to different purchases of the given user (but with a different similarity value with each purchase).\n",
        "\n",
        "That is, to have for a particular item j, that we want to consider as a possible recommendation, a set of different similarity values for different products:\n",
        "$$ (j,p_1,s_1), (j,p_2,s_2), \\ldots, (j,p_m,s_m)  $$\n",
        "if item j was found similar to different previous purchases $p_1,p_2,\\ldots,p_m$ of the user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIUAlpGHu-YD"
      },
      "source": [
        "So, we should **aggregate** in some way the different similarity values\n",
        "$$  s_1, s_2, \\ldots, s_m  $$\n",
        "obtained for a same item. How can we aggregate the different similarities between a given product and a set of different products ?\n",
        "\n",
        "- We could use the average/median value of the set similarity values for a same item\n",
        "- We could use some kind of weighted sum, that gives more relevance to items where the similarity info was computed using the info of more users\n",
        "$$ wSum =  \\sum_i w_i s_i  $$\n",
        "where $w_i$ represents the weight (relevance) of the similarity value $s_i$ towards computing the final value. Observe that setting $w_i=1/m$ gives the regular average value, that gives all the values the same relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uJLIsM9u-YE"
      },
      "source": [
        "# Measuring similarity in other ways\n",
        "\n",
        "As we have seen, the key component of this kind of recommender systems is the way to compute similarity between items. We have presented a very common one, the cosine distance, but there are some others, such as the Jaccard distance or the Pearson correlation coefficient.\n",
        "\n",
        "Here you have a good source of information about different ways to measure similarity for recommender systems:\n",
        "\n",
        "> \"A new user similarity model to improve the accuracy of collaborative filtering\" by Haifeng Liu, Zheng Hu, Ahmad Mian, Hui Tian, Xuzhen Zhu. https://doi.org/10.1016/j.knosys.2013.11.006\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNAMENT 2"
      ],
      "metadata": {
        "id": "6STokg17vCA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1"
      ],
      "metadata": {
        "id": "NJo7R7Y8vJfH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnHmDu_9vtO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2"
      ],
      "metadata": {
        "id": "v5Zg6utxwAzI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BrLXVfqDwJce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 3"
      ],
      "metadata": {
        "id": "f8Nmq7WbwKBS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbPLv2FSwNJs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}